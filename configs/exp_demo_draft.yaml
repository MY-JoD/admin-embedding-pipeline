# =========================
# IDENTITÉ EXPÉRIENCE
# =========================
exp_id: exp_UI_0001_demo
description: "Demo UI Fine-tuning incrémental d'un modèle E5 large multilingual  (pair positive) avec LoRA."

# =========================
# MODÈLE
# =========================
model:
  base_model_ref: "intfloat/multilingual-e5-large-instruct"   # adapte au nom exact HF ou chemin local
  tokenizer_ref: null
  strategy: "chain"                            # incrémental via dernier adapter
  precision: "bf16"
  device: "cuda"
  local_store_dir: "model_staging/base_models"   # dossier de persistance
  prefer_local: true
  allow_download: true
  local_id: "multilingual-e5-large-instruct"         # nom dossier local (stable)

# =========================
# TÂCHE EMBEDDING
# =========================
task:
  type: "embedding"
  format: "pair"                               # pair positive
  text_fields:
    query: "query"
    positive: "positive"


ui:
  asset_dir: "experiments/{exp_id}/assets_ui"

feeder:
  enabled: true
  provider: "simulation"          # simulation | ui_upload | manual
  max_iterations: 4               # iter01..iter04 après initial
  batch_filename: "batch.jsonl"

  simulation:
    sim_id: "sim_0001_term_to_def"
    root_dir: "staging_data/simulations"
    copy_ui_once: true

# =========================
# DONNÉES
# =========================
data:
  incoming_dir: "data_sources/{exp_id}/incoming"
  canonical_format: "jsonl"
  policy: "delta_plus_replay"                  # incrémental stable
  replay_ratio: 0.2
  seed: 42

# =========================
# ENTRAÎNEMENT
# =========================

training:
  enabled: true
  epochs: 1
  batch_size: 16
  max_seq_length: 256
  lr: 2e-4
  precision: "bf16"        # bf16 | fp16 | fp32
  device: "cuda"           # cuda | cpu
  save_every: "end"        # end (simple)

# =========================
# LORA
# =========================
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  bias: "none"
  target_modules: ["query", "key", "value"]


# =========================
# Replay Buffer
# =========================
replay:
  enabled: false
  mode: "ratio"              # ratio | fixed | full_mix | initial_only
  ratio: 0.2                 # replay_size = ratio * delta_size
  source: "initial_plus_recent"     # all_previous | initial_plus_recent | recent_only
  recent_k: 2                # utilisé si source inclut "recent"
  per_snapshot_cap: 0.5      # max 50% du replay venant d'un seul snapshot
  seed: 42
  # min_delta_size_for_replay: 200


# =========================
# INDEXATION
# =========================
indexing:
  enabled: true

  # on choisit la simulation UI à indexer
  ui:
    sim_id: "sim_0001_term_to_def"
    ui_dir: "experiments/{exp_id}/assets_ui/{sim_id}/ui"
    meta_path: "experiments/{exp_id}/assets_ui/{sim_id}/meta.json"
    file_suffix: ".rich.jsonl"   # tes fichiers
    subsets: ["initial", "iter01", "iter02", "iter03", "iter04"]  # optionnel; sinon auto

  schema:
    query_field: "query"
    text_field: "positive"   # texte vectorisé (definition dans ton exemple)
    label_field: "term"      # affichage top-k
    id_field: "source_row_id"  # ou "term" si tu veux; mieux: source_row_id si dispo
    keep_fields: ["term","query","positive","definition","prompt","pair_mode","k","source_row_id"]

  build:
    batch_size: 64
    max_length: 256
    normalize: true
    metric: "ip"             # cosine si normalize=true
    device: "cuda"
    precision: "bf16"


# =========================
# TRACKING
# =========================
tracking:
  local:
    enabled: true
  wandb:
    enabled: false




# =========================
# RUNTIME
# =========================
runtime:
  resume_if_interrupted: true



